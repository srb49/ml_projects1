{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "708c1283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re,nltk,json, pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import LSTM,GRU\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "np.random.seed(42)\n",
    "class color: # Text style\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c602161d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('abp.csv',encoding='utf-8')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "917add9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              label                                               body\n",
      "0     international  জঙ্গি হটাতে যুদ্ধের ডাক দিয়েছিলেন আগেই। এ বার ...\n",
      "1             state  শাসকদলের গোষ্ঠীকোন্দলের জেরে সোমবার চাষিদের নথ...\n",
      "2             state  শিক্ষার সর্বস্তরে শিক্ষক-শিক্ষিকাদের হাজিরায় ব...\n",
      "3          national  এখন সোশ্যাল মিডিয়া ছাড়া ভাবাই যায় না। কোনও ঘট...\n",
      "4             state  তদন্তের গতিপ্রকৃতি নিয়ে আদালতে প্রশ্ন ওঠার পর ...\n",
      "...             ...                                                ...\n",
      "1560       national  ওই রিপোর্ট জানাল, বছরদু’য়েক আগে যে নোটবন্দি অভ...\n",
      "1561          state  ফের পারদ পতন মহানগরে! হাওয়া অফিস সূত্রের খবর, ...\n",
      "1562          state  এই চমকপ্রদ ঘোষণা খোদ মুখ্যমন্ত্রী মমতা বন্দ্যো...\n",
      "1563          state  দাড়িভিটে গুলিবিদ্ধ হয়ে দুই ছাত্রের মৃত্যুর ঘট...\n",
      "1564          state  শুধু দেহের দীর্ঘ আয়ুষ্কাল কোন স্রষ্টারই বা কাঙ...\n",
      "\n",
      "[1565 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "data1 = data[['label','body']]\n",
    "print(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0751d588",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the Class distribution\n",
    "sns.set(font_scale=1.4)\n",
    "data['label'].value_counts().plot(kind='barh', figsize=(8, 6))\n",
    "plt.xlabel(\"Number of Articles\", labelpad=12)\n",
    "plt.ylabel(\"label\", labelpad=12)\n",
    "plt.yticks(rotation = 45)\n",
    "plt.title(\"Dataset Distribution\", y=1.02);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb4fbe0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample_data = [20,50,100,200,300,350,400,450,500,600,650,700,750,800]\n",
    "for i in sample_data:\n",
    "      print('body:\\n',data.body[i],'Category:-- ',data.label[i],'\\n')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99b8f69",
   "metadata": {},
   "outputs": [],
   "source": [
    " news = re.sub('[^\\u0980-\\u09FF]',' ',str(data.body[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d93f71e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77eb7dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-933c373ebc73>:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data1['cleaned'] = data1['body'].apply(cleaning_documents)\n"
     ]
    }
   ],
   "source": [
    "#########------------------------\n",
    "#############----------------------------------this one is not working\n",
    "# Cleaning Data [Remove unncessary symbols]\n",
    "def cleaning_documents(articles):\n",
    "      '''\n",
    "      This function will clean the news articles by removing punctuation marks and stopwords.\n",
    "      '''\n",
    "      news = re.sub('\\n',' ',str(articles))  \n",
    "     #removing unnecessary punctuation\n",
    "     #link: https://www.google.com/search?q=removing+puntuation+regular+expression+python&rlz=1C1CHWL_enIN894IN894&oq=removing+puntuation+regular+expression+python&aqs=chrome..69i57j33i22i29i30l2.13383j1j7&sourceid=chrome&ie=UTF-8#kpvalbx=_4H-8Y86lEKqOxc8PvrSx-A4_42\n",
    "      news = re.sub('[^\\u0980-\\u09FF]',' ',str(news))        \n",
    "      # stopwords removal\n",
    "      stp = open('stopwords.txt','r',encoding='utf-8').read().split()\n",
    "      result = news.split()\n",
    "      news = [word.strip() for word in result if word not in stp ]\n",
    "      news =\" \".join(news)\n",
    "      return news\n",
    "\n",
    "\n",
    "# Apply the function into the dataframe\n",
    "data1['cleaned'] = data1['body'].apply(cleaning_documents)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d2ef66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1565\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92aafc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\somrita\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3437: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# Cleaning Data [Remove unncessary symbols]\n",
    "def cleaning_documents(articles):\n",
    "      '''\n",
    "      This function will clean the news articles by removing punctuation marks and stopwords.\n",
    "      '''\n",
    "      news = re.sub('\\n',' ',str(articles))  \n",
    "     #removing unnecessary punctuation\n",
    "     #link: https://www.google.com/search?q=removing+puntuation+regular+expression+python&rlz=1C1CHWL_enIN894IN894&oq=removing+puntuation+regular+expression+python&aqs=chrome..69i57j33i22i29i30l2.13383j1j7&sourceid=chrome&ie=UTF-8#kpvalbx=_4H-8Y86lEKqOxc8PvrSx-A4_42\n",
    "      news = re.sub('[^\\u0980-\\u09FF]',' ',str(news))        \n",
    "      # stopwords removal\n",
    "      stp = open('stopwords.txt','r',encoding='utf-8').read().split()\n",
    "      result = news.split()\n",
    "      news = [word.strip() for word in result if word not in stp ]\n",
    "      news =\" \".join(news)\n",
    "      return news\n",
    "#cleaned=[]\n",
    "\n",
    "for i in range(len(data)):\n",
    "    data1.cleaned[i]=cleaning_documents(data1.body[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750173b2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('after change')\n",
    "for i in sample_data:\n",
    "      print('body:\\n',data1.cleaned[i],'Category:-- ',data1.label[i],'\\n')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "590475d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Length of each Document\n",
    "data1['Length'] = data1.cleaned.apply(lambda x:len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c71102c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       163\n",
      "1        67\n",
      "2       266\n",
      "3       161\n",
      "4       174\n",
      "       ... \n",
      "1560    128\n",
      "1561    174\n",
      "1562    463\n",
      "1563    161\n",
      "1564    388\n",
      "Name: Length, Length: 1565, dtype: int64\n",
      "23 0 \n",
      "343 0 \n",
      "433 23 পোস্টার লঞ্চে অভিনবত্য দেখাল তিন টিম পরিচালক ঋভু দাশগুপ্ত তিন টিম সমর্থদের গিয়েছিলেন অভিনব প্রতিযোগিতা প্রতিযোগিতার সাকসেস পার্টি গ্র্যান্ড পার্টিতে চোখ রাখুন গ্যালারিতে\n",
      "825 21 ভাবুন রাস্তা দিয়ে হাঁটছেন হঠাত্ই পাশ দিয়ে জল রাস্তায় ভরে জলের উচ্চতা সমান হ্যাঁ রকমই রাস্তা রয়েছে ফ্রান্সে জেনে নিন গ্যালারিতে\n",
      "967 24 মির্জা পরিবারে বিয়ের সানাই বাজল বিয়ে সানিয়া মির্জার বোন আনম মির্জা সঙ্গীত অনুষ্ঠানে হাজির টাউনের তাবড় তারকারা এক নজরে নিন আনমের বিয়ের সঙ্গীনুষ্ঠানের ঝলক\n",
      "1069 0 \n",
      "1227 24 অসম বয়স অসম বন্ধুত্ব এক প্রান্তে জ্যাক রাসেল কুকুর অপর প্রান্তে চিতাবাঘ খাওয়া ঘুম সবই একসঙ্গে অপরের গায়ে ওঠে খেলাও কেমন বন্ধুত্ব দেখুন ভিডিও\n",
      "1509 23 ম্যাক্সিম ম্যাগাজিনের সুপার মডেলকে চিনতে পারছেন নাহ হলিউডি অভিনেত্রীর কথা বলছি ভাল দেখলেই বুঝবেন বলিউডের পর্দা কাঁপানো নায়িকা চেনা চিনে নিন সঙ্গের গ্যালারি\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "################check1------->remove 0 length record\n",
    "'''\n",
    "print(data1['Length'])\n",
    "\n",
    "for i in range(len(data1)):\n",
    "    if data1.Length[i]<25:\n",
    "        print(i,data1.Length[i],data1.cleaned[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faeb209",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rc_file_defaults()\n",
    "frequency = dict()\n",
    "for i in data1.Length:\n",
    "    frequency[i] = frequency.get(i, 0)+1\n",
    "\n",
    "plt.figure(figsize = (6,4))\n",
    "plt.bar(frequency.keys(), frequency.values(), color=(0.2, 0.4, 0.6, 0.6))\n",
    "plt.xlim(0, 800)\n",
    "# in this notbook color is not working but it should work.\n",
    "plt.xlabel('Length of the Documents')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Length-Frequency Distribution')\n",
    "#plt.savefig(path+'len_dist.png',dpi = 1000,bbox_inches ='tight')\n",
    "plt.show()  \n",
    "print(f\"Maximum Length of a Document: {max(data1.Length)}\")\n",
    "print(f\"Minimum Length of a Document: {min(data1.Length)}\")\n",
    "print(f\"Average Length of a Document: {round(np.mean(data1.Length),0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "062e8d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    " class_label = [k for k,v in data1.label.value_counts().to_dict().items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f58f0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['national', 'state', 'international', 'entertainment', 'kolkata', 'sport']\n"
     ]
    }
   ],
   "source": [
    "print( class_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a42131b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class Name :  national\n",
      "Number of Documents:665\n",
      "Number of Words:138202\n",
      "Number of Unique Words:23475\n",
      "Most Frequent Words:\n",
      "\n",
      "এক\t933\n",
      "হয়েছে\t931\n",
      "মোদী\t735\n",
      "কথা\t650\n",
      "বিজেপি\t628\n",
      "বিরুদ্ধে\t485\n",
      "দাবি\t480\n",
      "প্রধানমন্ত্রী\t472\n",
      "কংগ্রেস\t449\n",
      "হয়ে\t433\n",
      "\n",
      "Class Name :  state\n",
      "Number of Documents:436\n",
      "Number of Words:108041\n",
      "Number of Unique Words:20765\n",
      "Most Frequent Words:\n",
      "\n",
      "এক\t798\n",
      "হয়েছে\t791\n",
      "রাজ্য\t491\n",
      "টাকা\t433\n",
      "পুলিশ\t427\n",
      "অভিযোগ\t420\n",
      "কথা\t408\n",
      "দাবি\t377\n",
      "রাজ্যের\t355\n",
      "হয়ে\t354\n",
      "\n",
      "Class Name :  international\n",
      "Number of Documents:227\n",
      "Number of Words:40799\n",
      "Number of Unique Words:11816\n",
      "Most Frequent Words:\n",
      "\n",
      "এক\t353\n",
      "হয়েছে\t242\n",
      "মার্কিন\t186\n",
      "কথা\t167\n",
      "দেশের\t158\n",
      "হয়ে\t158\n",
      "ভারত\t152\n",
      "ভারতের\t144\n",
      "দাবি\t141\n",
      "দিয়ে\t133\n",
      "\n",
      "Class Name :  entertainment\n",
      "Number of Documents:192\n",
      "Number of Words:23878\n",
      "Number of Unique Words:8506\n",
      "Most Frequent Words:\n",
      "\n",
      "ছবি\t215\n",
      "এক\t171\n",
      "ছবির\t126\n",
      "একটা\t111\n",
      "কথা\t108\n",
      "হয়েছে\t105\n",
      "ছবিতে\t104\n",
      "হয়ে\t93\n",
      "অভিনয়\t88\n",
      "সোশ্যাল\t68\n",
      "\n",
      "Class Name :  kolkata\n",
      "Number of Documents:25\n",
      "Number of Words:5334\n",
      "Number of Unique Words:2540\n",
      "Most Frequent Words:\n",
      "\n",
      "কলকাতা\t41\n",
      "এক\t39\n",
      "হয়েছে\t32\n",
      "জানান\t27\n",
      "বিশ্ববিদ্যালয়ের\t27\n",
      "খবর\t25\n",
      "তৈরি\t23\n",
      "উপাচার্য\t22\n",
      "পুলিশ\t21\n",
      "গ্যাস\t21\n",
      "\n",
      "Class Name :  sport\n",
      "Number of Documents:20\n",
      "Number of Words:4538\n",
      "Number of Unique Words:2347\n",
      "Most Frequent Words:\n",
      "\n",
      "এক\t37\n",
      "হয়ে\t29\n",
      "ভারতীয়\t29\n",
      "ম্যাচ\t27\n",
      "জাতীয়\t23\n",
      "গোল\t21\n",
      "খেলার\t20\n",
      "বিরুদ্ধে\t20\n",
      "কোচ\t20\n",
      "বল\t20\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#######################check2------->stopwords are not removed properly---diye/hoye\n",
    "\n",
    "'''\n",
    "def data_summary(dataset):\n",
    "  '''\n",
    "  This function will print the summary of the dataset\n",
    "\n",
    "  Args:\n",
    "  dataset: cleaned texts\n",
    "\n",
    "  returns:\n",
    "  documents: Number of document per class\n",
    "  words: Number of words per class\n",
    "  u_words: Number of unique words per class\n",
    "  class_label: name of the categories\n",
    "  '''\n",
    "  documents = []\n",
    "  words = []\n",
    "  u_words = []\n",
    "  #total_u_word = [word.strip().lower() for t in list(dataset.cleaned) for word in t.strip().split()]\n",
    "  # find class names\n",
    "  class_label = [k for k,v in data1.label.value_counts().to_dict().items()]\n",
    "  for label in class_label: \n",
    "    word_list = [word.strip().lower() for t in list(data1[data1.label==label].cleaned) for word in t.strip().split()]\n",
    "    counts = dict()\n",
    "    for word in word_list:\n",
    "      counts[word] = counts.get(word, 0)+1\n",
    "    # sort the dictionary of word list  \n",
    "    ordered = sorted(counts.items(), key= lambda item: item[1],reverse = True)\n",
    "    # Documents per class\n",
    "    documents.append(len(list(data1[data1.label==label].cleaned)))\n",
    "    # Total Word per class\n",
    "    words.append(len(word_list))\n",
    "    # Unique words per class \n",
    "    u_words.append(len(np.unique(word_list)))\n",
    "    print(\"\\nClass Name : \",label)\n",
    "    print(\"Number of Documents:{}\".format(len(list(data1[data1.label==label].cleaned))))  \n",
    "    print(\"Number of Words:{}\".format(len(word_list))) \n",
    "    print(\"Number of Unique Words:{}\".format(len(np.unique(word_list)))) \n",
    "    print(\"Most Frequent Words:\\n\")\n",
    "    for k,v in ordered[:10]:\n",
    "      print(\"{}\\t{}\".format(k,v))\n",
    "  return documents,words,u_words,class_label\n",
    "\n",
    "# pass the whole dataset into the function\n",
    "\n",
    "documents,words,u_words,class_names = data_summary(data1)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "150e0026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total Documents</th>\n",
       "      <th>Total Words</th>\n",
       "      <th>Unique Words</th>\n",
       "      <th>Class Names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>665</td>\n",
       "      <td>138202</td>\n",
       "      <td>23475</td>\n",
       "      <td>national</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>436</td>\n",
       "      <td>108041</td>\n",
       "      <td>20765</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>227</td>\n",
       "      <td>40799</td>\n",
       "      <td>11816</td>\n",
       "      <td>international</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>192</td>\n",
       "      <td>23878</td>\n",
       "      <td>8506</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>5334</td>\n",
       "      <td>2540</td>\n",
       "      <td>kolkata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>4538</td>\n",
       "      <td>2347</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Total Documents  Total Words  Unique Words    Class Names\n",
       "0              665       138202         23475       national\n",
       "1              436       108041         20765          state\n",
       "2              227        40799         11816  international\n",
       "3              192        23878          8506  entertainment\n",
       "4               25         5334          2540        kolkata\n",
       "5               20         4538          2347          sport"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_matrix = pd.DataFrame({'Total Documents':documents,\n",
    "                            'Total Words':words,\n",
    "                            'Unique Words':u_words,\n",
    "                            'Class Names':class_names})\n",
    "data_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01c8992",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sns.set()\n",
    "plt.figure(figsize =(10, 8))\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(data_matrix.iloc[:,0:3], annot=True,fmt=\"d\", linewidths=0.5,linecolor = 'Black',cmap = \"YlGnBu\",ax = ax)\n",
    "# labels, title and ticks\n",
    "ax.set_ylabel('Class Names') \n",
    "ax.set_title('Data Statistics')\n",
    "y_label = [\"Total Documents\", \"Total Words\", \"Unique Words\"] \n",
    "ax.xaxis.set_ticklabels(y_label, rotation=45); ax.yaxis.set_ticklabels(class_names, rotation=45);\n",
    "ax.xaxis.tick_top()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f955be63",
   "metadata": {},
   "outputs": [],
   "source": [
    " ################# Label Encoding Function ##################\n",
    "                   #===========================================================\n",
    "def label_encoding(dataset,bool):\n",
    "  '''\n",
    "  This function will convert categorical class names into a numeric label\n",
    "\n",
    "  Args:\n",
    "  dataset: A dataset\n",
    "  bool : a boolean (True or False)\n",
    "\n",
    "  returns:\n",
    "  labels: encoded labels \n",
    "  '''\n",
    "  le = LabelEncoder()\n",
    "  le.fit(data1.label)\n",
    "  encoded_labels = le.transform(data1.label)\n",
    "  labels = np.array(encoded_labels) # Converting into numpy array\n",
    "  class_names =le.classes_ ## Define the class names again\n",
    "  if bool == True:\n",
    "    print(color.BOLD+\"\\n\\t\\t\\t===== Label Encoding =====\"+color.END,\"\\nClass Names:-->\",le.classes_)\n",
    "  return labels  \n",
    "\n",
    " ################# Dataset Splitting Function ###############\n",
    "                   #=========================================================== \n",
    "\n",
    "def dataset_split(news,category):\n",
    "  '''\n",
    "  This function will split the dataset into Train-Test-Validation set\n",
    "\n",
    "  Args:\n",
    "  news: encoded texts\n",
    "  category: class names\n",
    "\n",
    "  returns:\n",
    "  X_train: Encoded Training News  \n",
    "  X_valid: Encoded Validation News\n",
    "  X_test:  Encoded Test news\n",
    "  y_train: Encoded Training labels\n",
    "  y_valid: Encoded Validation labels\n",
    "  y_test:  Encoded Test labels\n",
    "  '''\n",
    "\n",
    "  X,X_test,y,y_test = train_test_split(news,category,train_size = 0.9,\n",
    "                                                  test_size = 0.1,random_state =0)\n",
    "  X_train,X_valid,y_train,y_valid = train_test_split(X,y,train_size = 0.8,\n",
    "                                                  test_size = 0.2,random_state =0)\n",
    "  print(color.BOLD+\"Feature Size :======>\"+color.END,X_train.shape[1])\n",
    "  print(color.BOLD+\"\\nDataset Distribution:\\n\"+color.END)\n",
    "  print(\"\\tSet Name\",\"\\t\\tSize\")\n",
    "  print(\"\\t========\\t\\t======\")\n",
    "\n",
    "  print(\"\\tFull\\t\\t\\t\",news.shape[0],\n",
    "        \"\\n\\tTraining\\t\\t\",X_train.shape[0],\n",
    "        \"\\n\\tTest\\t\\t\\t\",X_test.shape[0],\n",
    "        \"\\n\\tValidation\\t\\t\",X_valid.shape[0])\n",
    "  \n",
    "  return X_train,X_valid,X_test,y_train,y_valid,y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6cb8f652",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\t\t\t====== Tokenizer Info ======\u001b[0m\n",
      "Words --> Counts:\n",
      "এক \t 2331\n",
      "হয়েছে \t 2108\n",
      "কথা \t 1363\n",
      "হয়ে \t 1087\n",
      "দাবি \t 1029\n",
      "পুলিশ \t 968\n",
      "অভিযোগ \t 914\n",
      "দিয়ে \t 888\n",
      "মোদী \t 873\n",
      "রয়েছে \t 859\n",
      "\n",
      "Words --> Documents:\n",
      "এক \t 1052\n",
      "হয়েছে \t 953\n",
      "কথা \t 736\n",
      "হয়ে \t 653\n",
      "দিয়ে \t 558\n",
      "দাবি \t 547\n",
      "রয়েছে \t 547\n",
      "গত \t 526\n",
      "যায় \t 503\n",
      "খবর \t 489\n",
      "\n",
      "Words --> Index:\n",
      "বলেওছিলেন \t 42407\n",
      "প্রয়াণকেই \t 42406\n",
      "জিনিসটার \t 42405\n",
      "বিনয়মাত্র \t 42404\n",
      "অমলকান্তি \t 42403\n",
      "কল্পনাপ্রবণ \t 42402\n",
      "সহনীয় \t 42401\n",
      "অভিঘাত \t 42400\n",
      "পরিবারমনস্ক \t 42399\n",
      "কোমলহৃদয় \t 42398\n",
      "\n",
      "Total Documents --> 1565\n",
      "\u001b[1m\n",
      "\t\t\t====== Encoded Sequences ======\u001b[0m \n",
      "Found 42407 unique tokens\n",
      "বুলন্দশহরের হত্যাকাণ্ডকে বিজেপি সভাপতি অমিত শাহ দুর্ভাগ্যজনক রাজস্থানে বিধানসভা ভোটের প্রচারের শেষ দিনে প্রধানমন্ত্রী নরেন্দ্র মোদী এক ঢিলে পাখি মারতে চেয়েছেন এক বুলন্দশহরের রাজনৈতিক মেরুকরণের সরাসরি ফায়দা রাজস্থানে চাইছেন উত্তরপ্রদেশের ঘটনাকে আইনশৃঙ্খলার অবনতি আখ্যা দিয়ে দোষী সাব্যস্ত মুখ্যমন্ত্রী যোগী আদিত্যনাথকে উত্তরপ্রদেশে বিধানসভা ভোট লোকসভার বাবরি মসজিদ ধ্বংসের তারিখ ৬ ডিসেম্বরের পরের দিনই রাজস্থানে ভোট গো বলয়ের রাজ্যটিতে মেরুকরণের ফায়দা তুলতে মরিয়া মোদী উত্তরপ্রদেশে বিজেপি শিবিরের অন্দরে যোগী বিরোধিতা তীব্র হয়ে উঠেছে মোদী যোগীকে মুখ্যমন্ত্রী চাননি মনোজ সিন্হার এক ভূমিহার নেতাকে মুখ্যমন্ত্রী রাজ্য নিয়ন্ত্রণে রাখতে চেয়েছিলেন যুক্তি ভূমিহারেরা উত্তরপ্রদেশে সংখ্যায় খুবই কম ভূমিহার মুখ্যমন্ত্রী সম্প্রদায়কে চলতে পারবেন সঙ্ঘ পরিবারের চাপে মোদীর কৌশল খাটেনি রাজ্য নেতাদের একাংশের অভিযোগ বর্তমান মুখ্যমন্ত্রী যোগী সন্ন্যাসী চেয়ে গোরক্ষপুরের ঠাকুর নেতা ঠাকুরদের আধিপত্য কায়েম হয়েছে পুলিশ প্রশাসনেও যোগী নাথ সম্প্রদায়ের কানফাট্টাইয়া সন্ন্যাসী সম্প্রদায় গোষ্ঠীর সন্ন্যাসীদের বিরোধী বিজেপি নেতাই ২০১৯ উত্তরপ্রদেশের মুখ্যমন্ত্রী বড় চমক দেখিয়ে লোকসভা ভোটে যাওয়ার পরামর্শ দিচ্ছেন মোদীকে ঘটনাই চাপ সৃষ্টি যোগীর দাদরির ঘটনা অখিলেশ যাদব মুখ্যমন্ত্রী বিজেপি দায় চাপিয়েছিল সমাজবাদী পার্টির মুজফ্ ফরনগরে জাঠ মুসলমান সংঘাত তখনও ভোটের ফায়দা তুলেছে বিজেপি কেন্দ্র উত্তরপ্রদেশ জায়গাতেই বিজেপি ক্ষমতায় সঙ্ঘ পরিবারের প্রশ্ন যোগী অযোধ্যা আবেগ ধরনের সংঘাতের আবহ তৈরি হিন্দুত্ববাদীদের সুসংহত ফেলেন ভোটের সরানো সম্ভব বিজেপিরই ক্ষতি মোদী আর্জেন্টিনা সকালে জয়পুরে সভায় হিন্দুত্বের সওয়াল অমিত শাহ বুলন্দশহরের ঘটনাকে দুর্ভাগ্যজনক বলার পাশাপাশিই অভিযোগ এক গভীর ষড়যন্ত্র বিজেপির বিরুদ্ধে ঘটনা রাজনীতি চাইছি বিরোধীরা উত্তরপ্রদেশে তদন্তকারী দল গঠন হয়েছে রিপোর্ট এলেই সত্য প্রকাশিত ঘটনার যাবতীয় যোগাযোগের অভিযোগ অস্বীকার বিজেপি বিশ্ব হিন্দু পরিষদের জাতীয় মুখপাত্র বিনোদ বনশল ঘটনার তীব্র নিন্দা করছি বদনাম প্রতিপক্ষ \n",
      " [1, 1, 12, 75, 177, 471, 3052, 3796, 398, 135, 1070, 38, 272, 23, 42, 10, 2, 1, 3797, 2565, 1118, 2, 1, 74, 3570, 274, 3053, 3796, 433, 571, 3186, 1, 4032, 3571, 9, 1539, 2126, 31, 1313, 1, 1009, 398, 97, 1792, 3572, 2780, 1, 1314, 606, 3054, 731, 472, 3796, 97, 1470, 1, 1, 3570, 3053, 691, 2049, 10, 1009, 12, 595, 2127, 1313, 831, 572, 5, 179, 10, 1, 31, 832, 3573, 1, 2, 1, 2119, 31, 22, 2381, 271, 1119, 287, 1, 1009, 4320, 755, 184, 1, 31, 1, 1095, 573, 1253, 172, 1048, 63, 952, 1, 22, 268, 704, 8, 817, 31, 1313, 1, 193, 1, 2660, 34, 1, 1, 4613, 3, 7, 1, 1313, 1, 2460, 1, 1, 1, 1115, 1, 87, 12, 4959, 1390, 571, 31, 57, 4033, 720, 169, 216, 107, 848, 542, 464, 4960, 473, 1708, 1, 1, 77, 1141, 705, 31, 12, 687, 1, 2200, 1619, 1, 1, 3798, 4321, 3799, 1540, 135, 3053, 1010, 12, 122, 1471, 3187, 12, 525, 1253, 172, 37, 1313, 1, 3574, 183, 1, 1, 26, 1, 1, 1750, 135, 2661, 163, 4614, 674, 10, 1, 191, 1, 638, 1, 953, 177, 471, 1, 3186, 3052, 914, 1, 8, 2, 990, 2781, 39, 13, 77, 717, 4615, 423, 1009, 805, 50, 1121, 3, 359, 4961, 3800, 1142, 117, 1096, 3358, 8, 1097, 12, 895, 1071, 919, 140, 436, 1, 1, 117, 572, 1391, 622, 1, 3801]\n",
      "\n",
      "\t\t\t====== Paded Sequences ======\n",
      " বুলন্দশহরের হত্যাকাণ্ডকে বিজেপি সভাপতি অমিত শাহ দুর্ভাগ্যজনক রাজস্থানে বিধানসভা ভোটের প্রচারের শেষ দিনে প্রধানমন্ত্রী নরেন্দ্র মোদী এক ঢিলে পাখি মারতে চেয়েছেন এক বুলন্দশহরের রাজনৈতিক মেরুকরণের সরাসরি ফায়দা রাজস্থানে চাইছেন উত্তরপ্রদেশের ঘটনাকে আইনশৃঙ্খলার অবনতি আখ্যা দিয়ে দোষী সাব্যস্ত মুখ্যমন্ত্রী যোগী আদিত্যনাথকে উত্তরপ্রদেশে বিধানসভা ভোট লোকসভার বাবরি মসজিদ ধ্বংসের তারিখ ৬ ডিসেম্বরের পরের দিনই রাজস্থানে ভোট গো বলয়ের রাজ্যটিতে মেরুকরণের ফায়দা তুলতে মরিয়া মোদী উত্তরপ্রদেশে বিজেপি শিবিরের অন্দরে যোগী বিরোধিতা তীব্র হয়ে উঠেছে মোদী যোগীকে মুখ্যমন্ত্রী চাননি মনোজ সিন্হার এক ভূমিহার নেতাকে মুখ্যমন্ত্রী রাজ্য নিয়ন্ত্রণে রাখতে চেয়েছিলেন যুক্তি ভূমিহারেরা উত্তরপ্রদেশে সংখ্যায় খুবই কম ভূমিহার মুখ্যমন্ত্রী সম্প্রদায়কে চলতে পারবেন সঙ্ঘ পরিবারের চাপে মোদীর কৌশল খাটেনি রাজ্য নেতাদের একাংশের অভিযোগ বর্তমান মুখ্যমন্ত্রী যোগী সন্ন্যাসী চেয়ে গোরক্ষপুরের ঠাকুর নেতা ঠাকুরদের আধিপত্য কায়েম হয়েছে পুলিশ প্রশাসনেও যোগী নাথ সম্প্রদায়ের কানফাট্টাইয়া সন্ন্যাসী সম্প্রদায় গোষ্ঠীর সন্ন্যাসীদের বিরোধী বিজেপি নেতাই ২০১৯ উত্তরপ্রদেশের মুখ্যমন্ত্রী বড় চমক দেখিয়ে লোকসভা ভোটে যাওয়ার পরামর্শ দিচ্ছেন মোদীকে ঘটনাই চাপ সৃষ্টি যোগীর দাদরির ঘটনা অখিলেশ যাদব মুখ্যমন্ত্রী বিজেপি দায় চাপিয়েছিল সমাজবাদী পার্টির মুজফ্ ফরনগরে জাঠ মুসলমান সংঘাত তখনও ভোটের ফায়দা তুলেছে বিজেপি কেন্দ্র উত্তরপ্রদেশ জায়গাতেই বিজেপি ক্ষমতায় সঙ্ঘ পরিবারের প্রশ্ন যোগী অযোধ্যা আবেগ ধরনের সংঘাতের আবহ তৈরি হিন্দুত্ববাদীদের সুসংহত ফেলেন ভোটের সরানো সম্ভব বিজেপিরই ক্ষতি মোদী আর্জেন্টিনা সকালে জয়পুরে সভায় হিন্দুত্বের সওয়াল অমিত শাহ বুলন্দশহরের ঘটনাকে দুর্ভাগ্যজনক বলার পাশাপাশিই অভিযোগ এক গভীর ষড়যন্ত্র বিজেপির বিরুদ্ধে ঘটনা রাজনীতি চাইছি বিরোধীরা উত্তরপ্রদেশে তদন্তকারী দল গঠন হয়েছে রিপোর্ট এলেই সত্য প্রকাশিত ঘটনার যাবতীয় যোগাযোগের অভিযোগ অস্বীকার বিজেপি বিশ্ব হিন্দু পরিষদের জাতীয় মুখপাত্র বিনোদ বনশল ঘটনার তীব্র নিন্দা করছি বদনাম প্রতিপক্ষ \n",
      " [   1    1   12   75  177  471 3052 3796  398  135 1070   38  272   23\n",
      "   42   10    2    1 3797 2565 1118    2    1   74 3570  274 3053 3796\n",
      "  433  571 3186    1 4032 3571    9 1539 2126   31 1313    1 1009  398\n",
      "   97 1792 3572 2780    1 1314  606 3054  731  472 3796   97 1470    1\n",
      "    1 3570 3053  691 2049   10 1009   12  595 2127 1313  831  572    5\n",
      "  179   10    1   31  832 3573    1    2    1 2119   31   22 2381  271\n",
      " 1119  287    1 1009 4320  755  184    1   31    1 1095  573 1253  172\n",
      " 1048   63  952    1   22  268  704    8  817   31 1313    1  193    1\n",
      " 2660   34    1    1 4613    3    7    1 1313    1 2460    1    1    1\n",
      " 1115    1   87   12 4959 1390  571   31   57 4033  720  169  216  107\n",
      "  848  542  464 4960  473 1708    1    1   77 1141  705   31   12  687\n",
      "    1 2200 1619    1    1 3798 4321 3799 1540  135 3053 1010   12  122\n",
      " 1471 3187   12  525 1253  172   37 1313    1 3574  183    1    1   26\n",
      "    1    1 1750  135 2661  163 4614  674   10    1  191    1  638    1\n",
      "  953  177  471    1 3186 3052  914    1    8    2  990 2781   39   13\n",
      "   77  717 4615  423 1009  805   50 1121    3  359 4961 3800 1142  117\n",
      " 1096 3358    8 1097   12  895 1071  919  140  436    1    1  117  572\n",
      " 1391  622    1 3801    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "\u001b[1m\n",
      "\t\t\t===== Label Encoding =====\u001b[0m \n",
      "Class Names:--> ['entertainment' 'international' 'kolkata' 'national' 'sport' 'state']\n",
      "\n",
      "Shape of Encoded Corpus =====> (1565, 300)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def encoded_texts(dataset,padding_length,max_words):\n",
    "\n",
    "  '''\n",
    "  This function will encoded the text into a sequence of numbers\n",
    "\n",
    "  Args:\n",
    "  dataset: a dataset\n",
    "  padding_length: maximum length of a encoded texte\n",
    "  max_words : Number of words \n",
    "\n",
    "  returns:\n",
    "  corpus: Number of encoded texts\n",
    "  labels: encoded labels\n",
    "  '''\n",
    "  \n",
    "  tokenizer = Tokenizer(num_words = max_words, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n-', \n",
    "                      split=' ', char_level=False, oov_token='', document_count=0)\n",
    "  \n",
    " \n",
    "\n",
    " # Fit the tokenizer\n",
    "  tokenizer.fit_on_texts(dataset.cleaned)\n",
    "  #============================== Tokenizer Info =================================\n",
    "  (word_counts,word_docs,word_index,document_count) = (tokenizer.word_counts,\n",
    "                                                       tokenizer.word_docs,\n",
    "                                                       tokenizer.word_index,\n",
    "                                                       tokenizer.document_count)\n",
    "  def tokenizer_info(mylist,bool):\n",
    "    ordered = sorted(mylist.items(), key= lambda item: item[1],reverse = bool)\n",
    "    for w,c in ordered[:10]:\n",
    "      print(w,\"\\t\",c)\n",
    "  #=============================== Print all the information =========================\n",
    "  print(color.BOLD+\"\\t\\t\\t====== Tokenizer Info ======\"+color.END)   \n",
    "  print(\"Words --> Counts:\")\n",
    "  tokenizer_info(word_counts,bool =True )\n",
    "  print(\"\\nWords --> Documents:\")\n",
    "  tokenizer_info(word_docs,bool =True )\n",
    "  print(\"\\nWords --> Index:\")\n",
    "  tokenizer_info(word_index,bool =True )    \n",
    "  print(\"\\nTotal Documents -->\",document_count)\n",
    "\n",
    "  #=========================== Convert string into list of integer indices =================\n",
    "  sequences = tokenizer.texts_to_sequences(dataset.cleaned)\n",
    "  word_index = tokenizer.word_index\n",
    "  print(color.BOLD+\"\\n\\t\\t\\t====== Encoded Sequences ======\"+color.END,\"\\nFound {} unique tokens\".format(len(word_index)))  \n",
    "  print(dataset.cleaned[10],\"\\n\",sequences[10]) \n",
    "\n",
    "  #==================================== Pad Sequences ============================== \n",
    "  corpus = keras.preprocessing.sequence.pad_sequences(sequences, value=0.0,\n",
    "                                                      padding='post', maxlen= padding_length)\n",
    "  print(\"\\n\\t\\t\\t====== Paded Sequences ======\\n\",dataset.cleaned[10],\"\\n\",corpus[10])   \n",
    "\n",
    "  #=================================     Label Encoding ================================\n",
    "  labels = label_encoding(dataset,True)\n",
    "\n",
    "   # save the tokenizer into a pickle file\n",
    "  with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "  return corpus,labels\n",
    "  \n",
    "#call the function\n",
    "num_words = 5000\n",
    "corpus,labels = encoded_texts(data1,\n",
    "                                300,num_words)\n",
    "print(\"\\nShape of Encoded Corpus =====>\",corpus.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0cd5e6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mFeature Size :======>\u001b[0m 300\n",
      "\u001b[1m\n",
      "Dataset Distribution:\n",
      "\u001b[0m\n",
      "\tSet Name \t\tSize\n",
      "\t========\t\t======\n",
      "\tFull\t\t\t 1565 \n",
      "\tTraining\t\t 1126 \n",
      "\tTest\t\t\t 157 \n",
      "\tValidation\t\t 282\n"
     ]
    }
   ],
   "source": [
    "X_train,X_valid,X_test,y_train,y_valid,y_test = dataset_split(corpus,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a089d7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "embedding_dimension = 128\n",
    "input_length = 300\n",
    "vocab_size = 5000\n",
    "num_classes = 12\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "accuracy_threshold = 0.97\n",
    "\n",
    "\n",
    "class myCallback(keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "      if(logs.get('accuracy')>accuracy_threshold):\n",
    "        print(\"\\nReached %2.2f%% accuracy so we will stop trianing\" % (accuracy_threshold*100))\n",
    "        self.model.stop_training = True\n",
    "\n",
    "acc_callback = myCallback()\n",
    "# Saved the Best Model\n",
    "filepath = \"Model.h5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=2, save_best_only=True, \n",
    "                                             save_weights_only=False, mode='max')\n",
    "# callback list\n",
    "callback_list = [acc_callback, checkpoint] \n",
    "\n",
    "                            ###############################     CNN network architecture ################\n",
    "model = tf.keras.models.Sequential([\n",
    "tf.keras.layers.Embedding(vocab_size, embedding_dimension, input_length = input_length),\n",
    "tf.keras.layers.Conv1D(128, 5, activation= 'relu'),\n",
    "tf.keras.layers.MaxPooling1D(5),\n",
    "tf.keras.layers.Bidirectional(LSTM(64, return_sequences=True,dropout = 0.2)),\n",
    "tf.keras.layers.Bidirectional(LSTM(64, return_sequences=True,dropout = 0.2)),\n",
    "tf.keras.layers.Dense(28, activation='relu'),\n",
    "tf.keras.layers.Dense(14, activation='relu'),\n",
    "keras.layers.Flatten(),\n",
    "tf.keras.layers.Dense(num_classes , activation='softmax')])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "   \n",
    "history = model.fit(X_train,\n",
    "    y_train,\n",
    "    epochs=num_epochs,\n",
    "    batch_size = batch_size,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    verbose =1)\n",
    "    #callbacks = callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c51be5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "tf.keras.models.save_model(model,'Document.h5', save_format='h5')\n",
    "#model.save(path+'Document_Categorization.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba89b48f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "loss_values = history.history['loss']\n",
    "val_loss_values = history.history['val_loss']\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "sns.set_style('darkgrid')\n",
    "                  #=========================================================\n",
    "                  ############### Epochs vs Validation Accuracy Plot ##########\n",
    "                  #==========================================================\n",
    "\n",
    "plt.plot(epochs, acc, color='midnightblue', linewidth = 2, \n",
    "          marker='o', markersize=8,label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, color='darkred', linewidth = 2, \n",
    "          marker='o', markersize=8,label='Training Accuracy')\n",
    "plt.title('Training and validation accuracy',fontsize=12,fontweight='bold')\n",
    "plt.xlabel('Epochs',fontsize=12,fontweight='bold')\n",
    "plt.ylabel('Accuracy',fontsize=12,fontweight='bold')\n",
    "plt.legend(['Training Accuracy','Validation Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f9b07755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "model = load_model('Document.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "65184ffe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 25ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\somrita\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\somrita\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\somrita\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>entertainment</th>\n",
       "      <td>85.71</td>\n",
       "      <td>57.14</td>\n",
       "      <td>68.57</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>international</th>\n",
       "      <td>39.29</td>\n",
       "      <td>40.74</td>\n",
       "      <td>40.00</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kolkata</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>national</th>\n",
       "      <td>66.20</td>\n",
       "      <td>92.16</td>\n",
       "      <td>77.05</td>\n",
       "      <td>51.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Education</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sport</th>\n",
       "      <td>86.05</td>\n",
       "      <td>75.51</td>\n",
       "      <td>80.43</td>\n",
       "      <td>49.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>68.15</td>\n",
       "      <td>68.15</td>\n",
       "      <td>68.15</td>\n",
       "      <td>0.681529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>46.21</td>\n",
       "      <td>44.26</td>\n",
       "      <td>44.34</td>\n",
       "      <td>157.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>66.58</td>\n",
       "      <td>68.15</td>\n",
       "      <td>66.18</td>\n",
       "      <td>157.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               precision  recall  f1-score     support\n",
       "entertainment      85.71   57.14     68.57   21.000000\n",
       "international      39.29   40.74     40.00   27.000000\n",
       "kolkata             0.00    0.00      0.00    5.000000\n",
       "national           66.20   92.16     77.05   51.000000\n",
       "Education           0.00    0.00      0.00    4.000000\n",
       "sport              86.05   75.51     80.43   49.000000\n",
       "accuracy           68.15   68.15     68.15    0.681529\n",
       "macro avg          46.21   44.26     44.34  157.000000\n",
       "weighted avg       66.58   68.15     66.18  157.000000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names =  ['entertainment' 'international' 'kolkata' 'national' 'sport' 'state']\n",
    "predictions = model.predict(X_test)\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "report = pd.DataFrame(classification_report(y_true = y_test, y_pred = y_pred, output_dict=True)).transpose()\n",
    "report = report.rename(index={'0':'entertainment','1':'international','2':'kolkata','3':'national','4':'Education','5':'sport','6':'state'})\n",
    "report[['precision','recall','f1-score']]=report[['precision','recall','f1-score']].apply(lambda x: round(x*100,2))\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc513de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "predictions = model.predict(X_test)\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred) \n",
    "\n",
    "class_names = ['entertainment' 'international' 'kolkata' 'national' 'sport' 'state']\n",
    "# Transform to df for easier plotting\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                     index = class_names, \n",
    "                     columns = class_names)\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm_df, annot=True,cmap=\"Greens\", fmt='g')\n",
    "plt.title('CNN-BiLSTM \\nAccuracy: {0:.2f}'.format(accuracy_score(y_test, y_pred)*100))\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.yticks(rotation = 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bd09f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_news = 'শিয়ালদহ স্টেশনে দাঁড়িয়ে থাকা লোকাল ট্রেনের ছাদ থেকে উদ্ধার হল এক অজ্ঞাতপরিচয় ব্যক্তির দেহ। ঘটনাটি ঘিরে চাঞ্চল্য ছড়ায় শিয়ালদহ দক্ষিণ শাখার স্টেশন চত্বরে। রেল পুলিশের ধারণা, সম্ভবত বিদ্যুৎস্পৃষ্ট হয়েই মৃত্যু হয়েছে ওই ব্যক্তির। এ দিন সকালে আপ ক্যানিং লোকাল শিয়ালদহ স্টেশনে এসে পৌঁছয়। তখনই ট্রেনের ছাদে এক ব্যক্তির দেহ নজরে আসে যাত্রী এবং আরপিএফ কর্মীদের। এর পরে ওই দেহটি উদ্ধার করা হয়। প্রাথমিকভাবে পুলিশকর্মীদের অনুমান, মানসিক ভারসাম্যহীন ওই ব্যক্তি কোনওভাবে ট্রেনে ছাদে উঠে পড়েছিলেন। এর পরে ট্রেনের ছাদের প্যান্টোগ্রাফের কাছে চলে যান '\n",
    "cleaned_news = cleaning_documents(sample_news)\n",
    "class_names = ['entertainment' 'international' 'kolkata' 'national' 'sport' 'state']\n",
    "\n",
    "#print(cleaned_news)\n",
    "# load the saved tokenizer\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    loaded_tokenizer = pickle.load(handle)\n",
    "#print(len(loaded_tokenizer.word_index))\n",
    "seq= loaded_tokenizer.texts_to_sequences([cleaned_news])\n",
    "padded = pad_sequences(seq, value=0.0,padding='post', maxlen= 500 )\n",
    "pred = model.predict(padded)\n",
    "#print(pred) \n",
    "class_names[np.argmax(pred)]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9177c304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1098"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str1=sample_news = ' নগদহীন লেনদেনের লক্ষ্যে এবার আরও একটা গুরুত্বপূর্ণ সিদ্ধান্ত নিল কেন্দ্র। কর্মীদের আর নগদে বেতন দেওয়া যাবে না। চেক বা ব্যাঙ্ক অ্যাকাউন্টের মাধ্যমে দেওয়া হবে বেতন। বুধবার কেন্দ্রীয় মন্ত্রিসভার বৈঠকে এমনই সিদ্ধান্ত গৃহীত হয়েছে। নোটবাতিলের পরেই জানা গিয়েছিল, নগদহীন লেনদেনকে উৎসাহিত করতে আরও নানা সিদ্ধান্ত নেবে মোদী সরকার। বুধবার কেন্দ্রীয় মন্ত্রিসভার বৈঠকে যে গুরুত্বপূর্ণ কোনও সিদ্ধান্ত হতে পারে সেটাও জানা ছিল। সেই মতো এদিন মন্ত্রিসভার বৈঠকে কর্মচারীদের নগদে বেতন দেওয়ার ক্ষেত্রে নিষেধাজ্ঞা জারির অর্ডিন্যান্সে সিলমোহর দেওয়া হয়েছে। এর ফলে চেকে বা সরাসরি কর্মচারীদের ব্যাঙ্ক অ্যাকাউন্টে বেতন দিতে হব ে। েখ্য, এ সংক্রান্ত বিল গত ১৫ ডিসেম্বর লোকসভায় পেশ করা হয়। এই বিল আগামী বাজেট অধিবেশনে পাস করানো হতে পারে। অর্থাৎ বিল পাস করাতে এখনও দু’মাস সময় লেগে যাবে। তাই এ বিষয়ে এখন অর্ডিন্যান্স জারি করা হল। পরে তা সংসদে পাস করানো হবে। নিয়ম অনুযায়ী, অর্ডিন্যান্স ছ’মাসের জন্য বৈধ থাকে। এর মধ্যে তা সংসদে অনুমোদন করতে হয়। বেতন প্রদান (সংশোধনী) বিল ২০১৬-তে মূল আইনের ৬ নম্বর ধারায় সংশোধনের প্রস্তাব করা হয়েছে, যাতে নিয়োগকারী কর্মচারীদের বেতন চেক বা ইলেকট্রনিক পদ্ধতিতে সরাসরি ব্যাঙ্ক অ্যাকাউন্টে পাঠাতে পারেন।'\n",
    "len(str1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
